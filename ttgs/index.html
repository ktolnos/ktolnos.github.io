<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title"
    content="Test-Time Graph Search for Goal-Conditioned Reinforcement Learning - Evgenii Opryshko*, Junwei Quan∗, Claas Voelker, Yilun Du, Igor Gilitschenski">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description"
    content="Test-Time Graph Search (TTGS) uses value-derived distances to plan subgoals over dataset states, boosting long-horizon GCRL without additional training.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Evgenii Opryshko*, Junwei Quan∗, Claas Voelker, Yilun Du, Igor Gilitschenski">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="TISL, University of Toronto">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Test-Time Graph Search for Goal-Conditioned Reinforcement Learning">
  <!-- TODO: Same as description above -->
  <meta property="og:description"
    content="Test-Time Graph Search (TTGS) uses value-derived distances to plan subgoals over dataset states, boosting long-horizon GCRL without additional training.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://ktolnos.github.io/ttgs/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt"
    content="Test-Time Graph Search for Goal-Conditioned Reinforcement Learning - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Evgenii Opryshko, Junwei Quan">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="offline reinforcement learning">
  <meta property="article:tag" content="graph search">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Test-Time Graph Search for Goal-Conditioned Reinforcement Learning">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description"
    content="Test-Time Graph Search (TTGS) uses value-derived distances to plan subgoals over dataset states, boosting long-horizon GCRL without additional training.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt"
    content="Test-Time Graph Search for Goal-Conditioned Reinforcement Learning - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Test-Time Graph Search for Goal-Conditioned Reinforcement Learning">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Test-Time Graph Search for Goal-Conditioned Reinforcement Learning - Evgenii Opryshko*, Junwei Quan∗, Claas
    Voelker, Yilun Du, Igor Gilitschenski | Academic Research</title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Test-Time Graph Search for Goal-Conditioned Reinforcement Learning",
    "description": "Test-Time Graph Search (TTGS) uses value-derived distances to plan subgoals over dataset states, boosting long-horizon GCRL without additional training.",
    "author": [
      {
        "@type": "Person",
        "name": "Evgenii Opryshko",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Toronto"
        }
      },
      {
        "@type": "Person",
        "name": "Junwei Quan",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Toronto, Vector Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Claas Voelker",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Toronto, Vector Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Yilun Du",
        "affiliation": {
          "@type": "Organization",
          "name": "Harvard University"
        }
      },
      {
        "@type": "Person",
        "name": "Igor Gilitschenski",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Toronto, Vector Institute"
        }
      }
    ],
    "datePublished": "2025-10-08",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://ktolnos.github.io/ttgs/",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "Offline goal-conditioned reinforcement learning (GCRL) trains policies that reach
    user-specified goals at test time, providing a simple, unsupervised, domain-agnostic
    way to extract diverse behaviors from unlabeled, reward-free datasets. Nonetheless,
    long-horizon decision making remains difficult for GCRL agents due to temporal
    credit assignment and error accumulation, and the offline setting amplifies these
    effects. To alleviate this issue, we introduce Test-Time Graph Search (TTGS), a
    lightweight planning approach to solve the GCRL task. TTGS accepts any state-space distance or cost signal, builds a
    weighted graph over dataset states, and performs fast search to assemble a sequence
    of subgoals that a frozen policy executes. When the base learner is value-based,
    the distance is derived directly from the learned goal-conditioned value function,
    so no handcrafted metric is needed. TTGS requires no changes to training, no
    additional supervision, no online interaction, and no privileged information, and it
    runs entirely at inference. On the OGBench benchmark, TTGS improves success
    rates of multiple base learners on challenging locomotion tasks, demonstrating the
    benefit of simple metric-guided test-time planning for offline GCRL.",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://ktolnos.github.io/ttgs/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "offline reinforcement learning"
      },
      {
        "@type": "Thing", 
        "name": "graph search"
      }
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "TISL, University of Toronto",
    "url": "https://tisl.cs.toronto.edu/",
    "logo": "https://tisl.cs.toronto.edu/icons/LongSiteLogo.svg",
    "sameAs": [
      "https://x.com/igilitschenski",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>

<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>
  <!--
  <!- - More Works Dropdown - ->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!- - TODO: Replace with your lab's related works - ->
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            <!- - TODO: Replace with actual paper title - ->
            <h5>Paper Title 1</h5>
            <!- - TODO: Replace with brief description - ->
            <p>Brief description of the work and its main contribution.</p>
            <!- - TODO: Replace with venue and year - ->
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!- - TODO: Add more related works or remove extra items - ->
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>
-->
  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <!-- TODO: Replace with your paper title -->
              <h1 class="title is-1 publication-title">Test-Time Graph Search for Goal-Conditioned Reinforcement
                Learning</h1>
              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your paper authors and their personal links -->
                <span class="author-block">
                  <a href="https://tisl.cs.toronto.edu/people/Evgenii%20Opryshko" target="_blank">Evgenii
                    Opryshko</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=oELpqRUAAAAJ&hl=en" target="_blank">Junwei
                    Quan</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://cvoelcker.de/" target="_blank">Claas Voelcker</a>,
                </span>
                <span class="author-block">
                  <a href="https://yilundu.github.io/" target="_blank">Yilun Du</a>,
                </span>
                <span class="author-block">
                  <a href="https://www.gilitschenski.org/igor/" target="_blank">Igor Gilitschenski</a>
                </span>

              </div>

              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your institution and conference/journal info -->
                <span class="author-block">University of Toronto, Vector Institute, Harvard
                  University<br><!--Conference name and year--></span>
                <!-- TODO: Remove this line if no equal contribution -->
                <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2510.07257.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- TODO: Add your supplementary material PDF or remove this section -->
                  <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/ktolnos/TTGS" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2510.07257" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser has-text-justified">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <p>
            <img src="static/images/TTGS_teaser.svg" alt="TTGS overview" class="blend-img-background center-image"
              style="width: 100%; max-width: 100%; height: auto;">
          </p>
          <br>
          <p>
            From an offline dataset, we sample observations to form graph vertices. We assign edge weights using a
            distance signal, either derived from a pretrained goal-conditioned value function or from domain-specific
            knowledge. A shortest-path search with Dijkstra's algorithm yields a sequence of subgoals that guides a
            frozen policy at test time.
          </p>
        </div>
      </div>
    </section>


    <!-- Teaser video-->
    <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!- - TODO: Replace with your teaser video - ->
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        <!- - TODO: Add your video file path here - ->
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video>
      <!- - TODO: Replace with your video description - ->
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <!-- TODO: Replace with your paper abstract -->
              <p>
                Offline goal-conditioned reinforcement learning (GCRL) trains policies that reach user-specified goals
                at test time, providing a simple, unsupervised, domain-agnostic way to extract diverse behaviors from
                unlabeled, reward-free datasets. Nonetheless, long-horizon decision making remains difficult for GCRL
                agents due to temporal credit assignment and error accumulation, and the offline setting amplifies these
                effects. To alleviate this issue, we introduce Test-Time Graph Search (TTGS), a lightweight planning
                approach to solve the GCRL task. TTGS accepts any state-space distance or cost signal, builds a weighted
                graph over dataset states, and performs fast search to assemble a sequence of subgoals that a frozen
                policy executes. When the base learner is value-based, the distance is derived directly from the learned
                goal-conditioned value function, so no handcrafted metric is needed. TTGS requires no changes to
                training, no additional supervision, no online interaction, and no privileged information, and it runs
                entirely at inference. On the OGBench benchmark, TTGS improves success rates of multiple base learners
                on challenging locomotion tasks, demonstrating the benefit of simple metric-guided test-time planning
                for offline GCRL.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!-- Image carousel -->
    <!--  
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images - ->
        <img src="static/images/carousel1.jpg" alt="First research result visualization" loading="lazy"/>
        <!- - TODO: Replace with description of this result - ->
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <!- - Your image here - ->
        <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <!- - Your image here - ->
        <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <!- - Your image here - ->
      <img src="static/images/carousel4.jpg" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
    <!-- End image carousel -->






    <!-- Paper poster -->
    <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <!- - TODO: Replace with your poster PDF - ->
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
    <!--End paper poster -->
    <section class="section hero is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content has-text-justified">
              <h2 class="title is-3">Motivation and Method</h2>
              <p>
                We visualize the core challenge that GCRL agents face.
                In long, complex tasks like maze navigation, agents that attempt to reach far-away goals can get stuck
                or run off course.
                However, for shorter horizons, they tend to be reliable.
              </p>

              <p>
                From this observation, we derive the simple key idea behind TTGS: rather than asking a policy to solve a
                long-horizon task in one shot, we decompose it into short, reliable hops.
                We do so by equipping test-time planning with a distance signal over states, selecting a compact set of
                representative states from an offline dataset, and connecting them into a graph whose edges reflect
                predicted step costs. At test time, we plan in this graph by computing a shortest path between the start
                and the goal, then feed the policy a small number of intermediate subgoals along this path. The agent
                queries all points along the shortest path that are within a certain threshold from the current state,
                and picks the closest to the goal.
              </p>
            </div>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="is-half">
            <p>
              <img src="static/images/base_overlay.svg" alt="Main results" class="blend-img-background center-image"
                style="width: 100%; height: auto; align-self: center;">
            </p>
          </div>
          <div style="width: 20px;"></div>
          <div class="is-half">
            <p>
              <img src="static/images/ttgs_overlay.svg" alt="Main results" class="blend-img-background center-image"
                style="width: 100%; height: auto; align-self: center;">
            </p>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-full">
            <div class="content">
              <p>
                <img src="static/images/dist_eval.svg" alt="Main results" class="blend-img-background center-image"
                  style="width: 100%; height: auto; align-self: center;">
              </p>
            </div>
          </div>
        </div>
        <div class="columns is-centered has-text-justified">
          <div class="column is-four-fifths">
            <div class="content container">
              <p>
                <i>(left)</i> HIQL policy fails to reach a distant goal on <i>antmaze-giant-stitch-v0</i>, with
                multiple
                attempts failing to exit the starting area and two attempts running out of time due to inefficient
                path.
                <i>(right)</i> TTGS finds a guiding path using dataset observations. On each step it selects a subgoal
                which is within a predefined radius from the agent. We mark all data points on the guiding path in
                gray,
                and the actual path traversed by the agent in blue. <i>(bottom)</i> Different agents' policy
                performance
                decreases as steps required to reach the goal increase. By providing a policy with close subgoals,
                TTGS
                improves reliability and efficiency of reaching the goal.
              </p>
            </div>
          </div>
        </div>
        <div class="columns is-centered has-text-justified">
          <div class="column">
            <div class="content">
              <p>
                Using the distance predictor, which for value-based base agents can be derived by mapping predicted
                values into distance, we build a graph with vertices sampled from the states of pretraining dataset and
                weights obtained by clipping the distances and applying a superlinear penalty to long connections. This
                graph is goal-agnostic, which means it can be reused for different goals
                an agent might want to achieve in the environment.
              </p>
              <p>
                To find the path to a given goal, the agent first precomputes the shortest path from the vertex closest
                to the starting state to the vertex closest to the goal state. After that, the agent selects subgoals
                from this guiding path which are within distance T from current step, or next after the closest vertex
                on the path if all other potential subgoals are too far.
              </p>
            </div>
          </div>
        </div>
      </div>
      </div>
    </section>
    <section class="section hero is-small is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-justified">
          <div class="column is-full">
            <div class="content">
              <h2 class="title is-3">Results</h2>
              <p>
                We evaluate TTGS on <a class="has-text-primary" href="https://seohong.me/projects/ogbench/">OGBench</a> using three strong
                offline GCRL baselines from the benchmark: <a class="has-text-primary" href="https://arxiv.org/abs/2304.01203">QRL</a>, <a
                  class="has-text-primary" href="https://arxiv.org/abs/2110.06169">GCIQL</a>, and <a
                  class="has-text-primary" href="https://seohong.me/projects/hiql/">HIQL</a>. For each dataset we compare the success rate of the
                frozen base policy with and without TTGS. TTGS uses distances derived from the base agent’s value
                function, so it relies only on information available at test time. TTGS generally improves or maintains
                success across all evaluated environments, with the largest gains on <span class="is-family-monospace">giant</span> layouts where one-shot
                execution is difficult.
              </p>
              <img src="static/images/ttgs_bars.svg" alt="Main results" class="blend-img-background center-image"
                style="width: 100%; height: auto; align-self: center;">
            </div>
          </div>
        </div>
      </div>
    </section>

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@misc{opryshko2025testtimegraphsearchgoalconditioned,
      title={Test-Time Graph Search for Goal-Conditioned Reinforcement Learning}, 
      author={Evgenii Opryshko and Junwei Quan and Claas Voelcker and Yilun Du and Igor Gilitschenski},
      year={2025},
      eprint={2510.07257},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2510.07257}, 
}</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                You are free to borrow the source code of this website, we just ask that you link back to this page in
                the footer. <br> This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>